{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparing Public School Funding and Mortality Rates Due to Drug Abuse and Self-Harm In Ohio\n",
    ">**CS3900 Final Project**\n",
    "\n",
    ">Jonathon Gebhardt, Brian Duffy, Alexander Silcott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header.jpg\" />\n",
    "Source: https://www.1and1.com/digitalguide/online-marketing/web-analytics/what-is-machine-learning-how-machines-learn-to-think/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "It is common knowledge that in recent years there has been an increase in drug-related deaths--especially in Ohio. There are any number of reasons for a spike in this type of mortality rate which can include economic and environmental factors. Self worth is also usually drawn from these factors. Educated individuals with a better school experience might be deterred from engaging in behaviors that would result in a death by overdose or suicide than individuals with a poor school experience (i.e. no art or after school programs, poor environment in class).\n",
    "\n",
    "**We hope to address the following questions:**\n",
    "- Is there a relationship between the amount of funding public schools get and the mortality rates of individuals in those areas due to drug abuse or self-harm?\n",
    "\n",
    "- Can we predict if a change to a school district’s budget will have an effect on the mortality rate of a particular area?\n",
    "\n",
    "- Are there specific areas school’s can spend money on that can reduce these mortality rates and perhaps help general public health as a result?\n",
    "\n",
    "## Files included\n",
    "\n",
    "### Python scripts\n",
    "- **csvconvert.py** - Convert given xlsx file to csv.\n",
    "- **preprocess.py** - Preprocess csv files. Find intersection and complement of our datasets so we can build a combined dataset using a foreign key.\n",
    "- **trim.py** - Trim off extra columns we don't need reduce features in data.\n",
    "\n",
    "### Datasheets and other stuff\n",
    "- **jupyter-notebook.ipynb** - Jupyter notebook to present information.\n",
    "- **grad.csv** - Graduation information about school districts. Used to cross-reference IRNs to get collated data.\n",
    "- **district.csv** - School expenditure information for Ohio.\n",
    "- **mort.csv** - Average mortality rate by county. (Deaths per 100,000)\n",
    "- **expanded.csv** - All of the schools combined, before finding intersection and complement.\n",
    "- **expanded_complement.csv** - All of the schools not in the intersection.\n",
    "- **expanded_intersection.csv** - Combined dataset before trimming columns.\n",
    "- **expanded_intersection_trimmed.csv** - Final combined dataset after trimming columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csvconvert.py\n",
    "**To begin, we created a script csvconvert.py which converts our input xlsx files into csv files.**\n",
    "It can be run from the command line, taking the file name of the xlsx file, the name of the desired sheet, followed by the output filename of the csv. In this python-friendly form, we can interpret and manipulate them inside of Sci-kit and Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load csvconvert.py\n",
    "#!/usr/bin/env: python3\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/20105118/convert-xlsx-to-csv-correctly-using-python\n",
    "\n",
    "\n",
    "# TODO: Don't make a sheet name necessary. If none is provided, assume there's only one sheet.\n",
    "# TODO: Handle errors with sheet name gracefully\n",
    "# ValueError: Sheet name is not in list\n",
    "# XLRDError: No sheet named ... \n",
    "\n",
    "\n",
    "import xlrd\n",
    "import csv\n",
    "import argparse\n",
    "import os.path\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"file_name\", help=\"name of excel workbook to be converted\")\n",
    "    parser.add_argument(\"sheet_name\", help=\"name of excel workbook sheet to be converted\")\n",
    "    parser.add_argument(\"output_file_name\", help=\"name of output file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "    if os.path.isfile(args.file_name):\n",
    "        wb = xlrd.open_workbook(args.file_name)\n",
    "        sh = wb.sheet_by_name(args.sheet_name)\n",
    "        your_csv_file = open(args.output_file_name, 'w')\n",
    "        wr = csv.writer(your_csv_file, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "        for rownum in range(sh.nrows):\n",
    "            wr.writerow(sh.row_values(rownum))\n",
    "\n",
    "        your_csv_file.close()\n",
    "\n",
    "    else:\n",
    "        print(\"Error: File \" + args.file_name + \" does not exist\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What features can we eliminate from the dataset?\n",
    "\n",
    "We have several options for features, and more than what we really need. Some of these columns have virtually no data in them for most schools. These are ones we can eliminate immediately based on this issue with zero values. This will help with selecting features from our dataset.\n",
    "\n",
    "**Adult Ed** - Although there might be some kind of correlation we can draw from this feature, there are many rows in which this has no data.\n",
    "\n",
    "**Instr Equipment** - Many rows with no data for this field; not relevant.\n",
    "\n",
    "**Land & Structures** - Many rows with no data for this field; not relevant.\n",
    "\n",
    "### Other features that can be eliminated?\n",
    "\n",
    "**Community Service** - We feel that this could play a role, but unfortuantely there are too many rows with zeros for this field.\n",
    "\n",
    "**Construction** - Not relevant to topic. Not all schools had expenditures in construction at this time.\n",
    "\n",
    "**Debt & Interest** - Not relevant to topic. Although this will have an impact on school spending, it most likely does not have impact on topic.\n",
    "\n",
    "**Enterprise** - Not relevant to topic. Expenditures may bee too broad to really have influence on topic.\n",
    "\n",
    "**Food Service** - This features is probably not relevant to our topic.\n",
    "\n",
    "**Org Type** - This feature can be removed because all of our datapoints are public districts and this column is redundant.\n",
    "\n",
    "**Other Equipment** - This contains expenditures of non-instructional expenditures, however the rows are inconsistent with zero values.\n",
    "\n",
    "**Pupil Transp** - This features is probably not relevant to our topic.\n",
    "\n",
    "**Weighted ADM** - This is the weighted average daily membership, which is important for the dataset but is not a necesarry part of our process as it was already used to determine values in the rows of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Index\n",
    "\n",
    "Now that we have reduced the number of features to use in our algorithm, we will define them as follows:\n",
    "\n",
    "**CRI - Classroom Instr** - Classroom instructional cost. Actual amount spent on classroom instructional purposes.\n",
    "\n",
    "**CRI%** - Percent spent on classrom Instructional purposes.\n",
    "\n",
    "**County Mortality Rate** - Average mortality rate (deaths per 100,000) for drug overdose in 2014 in the school's county.\n",
    "\n",
    "**County Name** - Name of the county that the school belongs to.\n",
    "\n",
    "**Gen Admin** - General Administration. Expenditure for board of education and executive administration (office of superintendent) services.\n",
    "\n",
    "**IRN** - Information Retrieval Number. Identification number assigned to educational entity. We use this ID to compare our data across multiple datasets.\n",
    "\n",
    "**Instr Staff Sup** - Instructional staff support services. Expenditures for supervision of instruction service improvements, curriculum development, instructional staff training, academic assessment, and media, library, and instruction-related technology services.\n",
    "\n",
    "**Instruction** - Activities dealing with the interaction of teachers and students in the classroom, home, or hospital as well as co-curricular activities. Includes teachers and instructional aides or assistants engaged in regular instruction, special education, and vocational education programs. Excludes adult education programs.\n",
    "\n",
    "**Local Education Agency Name** - Name of the entity/school, used for identification purposes.\n",
    "\n",
    "**NCR -Nonclassroom** - Nonclassroom expenditures. This includes general administration, school administration, other and non-specified support services, opearation and maintenance of plant, pupil transportation, and Elem-Sec Noninstructional Food service.\n",
    "\n",
    "**NCR%** - Percent spent on nonclassroom expenditures.\n",
    "\n",
    "**Non-Operating** - Non-Operating expenditures. The sum of enterprise operations, non-instructional--Other, community services, adult aducation, non-elementary-secondary programs--Other, construction, land and existing structures, equipment (instructional and other), and payment to other governments and interest on debt.\n",
    "\n",
    "**Oper & Maint** - Operation and maintenance of plant. Expenditure for buildings services (electricity, heating, air, insurance), care and upkeep of grounds and equipment, nonstudent transportation and maintenance; security devices.\n",
    "\n",
    "**Operating EPEP*** - Operating expenditures per equivalent pupil. Amount spent per pupil on operating cost.\n",
    "\n",
    "**Operating Expenditures** - Cost of instruction and support services, as well as administration and pupil transportation and food services. We left out a few of these metrics and believe we can use this value in lieu of them.\n",
    "\n",
    "**Other Elem-Sec** - Other Elementary-secondary Noninstructional. Expenditure for other elementary-secondary non-instructional activities not related to food services or enterprise operations.\n",
    "\n",
    "**Other Non Elem-Sec** - Other Nonelementary-secondary Programs . All other nonelementary-secondary programs such as any post-secondary programs for adults.\n",
    "\n",
    "**Other Support** - Other and Non-specified Support Services.  Business support expenditures for fiscal services (budgeting, receiving and disbursing funds, payroll, internal auditing, and accounting), purchasing, warehousing, supply distribution, printing, publishing, and duplicating services. Also include central support expenditures for planning, research and development, evaluation, information, management services, and expenditures for other support services not included elsewhere.\n",
    "\n",
    "**Pupil Support** - Pupil support Services. Expenditures for administrative, guidance, health, and logistical support that enhance instruction. Includes attendance, social work, student accounting, counseling, student appraisal, information, record maintenance, and placement services. Also includes medical, dental, nursing, psychological, and speech services.\n",
    "\n",
    "**School Admin** - School Administration. Expenditure for the office of the principal services.\n",
    "\n",
    "### *Note about EPEP\n",
    "EPEP (Expenditure per Equivalent Pupil) is similar to EPP (Expenditure Per Pupil). EPP considers all pupils equal whereas EPEP has a weighted value associated with it to make it more representative of the students actually attending the district (i.e. takes into account students who attent multiple schools in the school year).\n",
    "\n",
    "Source: http://education.ohio.gov/Topics/Finance-and-Funding/Finance-Related-Data/Expenditure-and-Revenue/Expenditure-Per-Pupil-Rankings\n",
    "\n",
    "Source: https://education.ohio.gov/getattachment/Topics/Data/Report-Card-Resources/Financial-Data/Technical-Guidance-Finance.pdf.aspx\n",
    "\n",
    "Source: http://www.tccsa.net/sites/tccsa.net/files/files/EMIS_Forms/Acronyms-EMIS.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trim.py\n",
    "\n",
    "**trim.py cross-references the IRN (information retrieval) numbers from our datasets and combines them into one file.**\n",
    "The next script relies on the output csv from this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load trim.py\n",
    "import csv\n",
    "\n",
    "input_variables = [\n",
    "    'Attendance 2012-13', 'Attendance 2010-11', \n",
    "    'Other Elem-Sec', 'Attendance 2011-12', \n",
    "    'District Class 2 Effective Millage Incl JVS FY14', \n",
    "    'District Percent Of Students In Poverty FY13', \n",
    "    'District Local Revenue As % Of Total FY13', \n",
    "    'Performance Index Score 2012-13', \n",
    "    'District Average Income TY11', 'Adult Ed', \n",
    "    'District Median Income TY11'\n",
    "    ]\n",
    "\n",
    "target_variables = [\n",
    "    'County Mortality Rate', 'Mortality Level'\n",
    "    ]\n",
    "\n",
    "def get_index(column_name, headers):\n",
    "    # Iterate through headers of data set looking for column \n",
    "    # that contains IRNs\n",
    "\n",
    "    for i in range(len(headers)):\n",
    "        if column_name in headers[i]:\n",
    "            return i\n",
    "\n",
    "def get_headers(file_name):\n",
    "    # Get the column names from the data set so that we can append \n",
    "    # them to the new data set\n",
    "    d_reader = csv.DictReader(csvfile)\n",
    "    headers = d_reader.fieldnames\n",
    "\n",
    "    return headers\n",
    "\n",
    "# Read file, append each line to a list, and return list\n",
    "def get_file_contents(file):\n",
    "    contents = []\n",
    "    reader = csv.reader(file, delimiter=',', quotechar='\"')\n",
    "\n",
    "    for row in reader:\n",
    "        contents.append(row)\n",
    "\n",
    "    return contents\n",
    "\n",
    "# We will base our merge on the lrc data set since it contains both \n",
    "# counties and LRCs\n",
    "with open('Converted_Datasets/lrc.csv', newline='') as csvfile:\n",
    "    lrc_headers = get_headers(csvfile)\n",
    "    lrc_contents = get_file_contents(csvfile)\n",
    "\n",
    "#expanded_contents = []\n",
    "with open('Converted_Datasets/expanded.csv', newline='') as csvfile:\n",
    "    expanded_headers = get_headers(csvfile)\n",
    "    expanded_contents = get_file_contents(csvfile)\n",
    "\n",
    "with open('Converted_Datasets/cupp.csv', newline='') as csvfile:\n",
    "    cupp_headers = get_headers(csvfile)\n",
    "    cupp_contents = get_file_contents(csvfile)\n",
    "\n",
    "with open('Converted_Datasets/mort.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "    count = 0\n",
    "    ohio_mortality_rates = []\n",
    "    for row in reader:\n",
    "        \n",
    "        if count == 2170:\n",
    "            break\n",
    "\n",
    "        if count > 2081:\n",
    "            county = row[0].split('County, ')[0]\n",
    "            rate = row[9].split(' ')[0]\n",
    "            \n",
    "            #0-12, 12-24, 24+\n",
    "            if float(rate) < 12:\n",
    "                mortality_category = 'low'\n",
    "            elif float(rate) < 24:\n",
    "                mortality_category = 'med'\n",
    "            else:\n",
    "                mortality_category = 'high'\n",
    "\n",
    "            ohio_mortality_rates.append([county, rate, mortality_category])\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "# Before we can merge, we need to get and merge headers from each data set\n",
    "all_headers = []\n",
    "for header in lrc_headers:\n",
    "    all_headers.append(header)\n",
    "\n",
    "for header in expanded_headers:\n",
    "    all_headers.append(header)\n",
    "\n",
    "for header in cupp_headers:\n",
    "    all_headers.append(header)\n",
    "\n",
    "# We need to get the IRN row so that we can match each row up\n",
    "lrc_irn_column = get_index('IRN', lrc_headers)\n",
    "cupp_irn_column = get_index('IRN', cupp_headers)\n",
    "expanded_irn_column = get_index('IRN', expanded_headers)\n",
    "\n",
    "# Now we can merge all the datasets. We want to skip the first row and stop \n",
    "# early to avoid weird casting issues.\n",
    "collosus = []\n",
    "collosus.append(all_headers)\n",
    "for lrc_row in lrc_contents[1:len(lrc_contents)-2]:\n",
    "    # Get current row IRN\n",
    "    lrc_row_irn = int(float(lrc_row[lrc_irn_column]))\n",
    "\n",
    "    # Iterate through other data sets looking for current row IRN\n",
    "    # Once found, append each entry to row\n",
    "    for expanded_row in expanded_contents[1:]:\n",
    "        if int(float(expanded_row[expanded_irn_column])) == lrc_row_irn:\n",
    "            for entry in expanded_row:\n",
    "                lrc_row.append(entry)\n",
    "            break\n",
    "\n",
    "    for cupp_row in cupp_contents[2:]:\n",
    "        if int(float(cupp_row[cupp_irn_column])) == lrc_row_irn:\n",
    "            for entry in cupp_row:\n",
    "                lrc_row.append(entry)\n",
    "            break\n",
    "\n",
    "    # Now append row to big data set\n",
    "    collosus.append(lrc_row)\n",
    "\n",
    "# Get indices of input variables\n",
    "input_variables_indices = []\n",
    "\n",
    "for variable in input_variables:\n",
    "    for i in range(len(all_headers)):\n",
    "        if all_headers[i] == variable:\n",
    "            input_variables_indices.append(i)\n",
    "            break\n",
    "\n",
    "# Build list of headers\n",
    "trimmed_dataset_headers = ['IRN', 'County']\n",
    "for input_variable in input_variables:\n",
    "    trimmed_dataset_headers.append(input_variable)\n",
    "\n",
    "trimmed_dataset_headers.append('County Mortality Rate')\n",
    "trimmed_dataset_headers.append('County Mortality Category')\n",
    "\n",
    "trimmed_dataset = []\n",
    "trimmed_dataset.append(trimmed_dataset_headers)\n",
    "\n",
    "# Iterate big data set adding only columns of input variables\n",
    "for row in collosus[1:]:\n",
    "    # Use a try catch to omit rows that are missing information\n",
    "    try:\n",
    "        row_irn = row[0]\n",
    "        row_county = row[2]\n",
    "        \n",
    "        temp = []\n",
    "        temp.append(row_irn)\n",
    "        temp.append(row_county)\n",
    "\n",
    "        # Append all target variables\n",
    "        for index in input_variables_indices:\n",
    "            temp.append(row[index])\n",
    "\n",
    "        # Append mortality information\n",
    "        for county in ohio_mortality_rates:\n",
    "            if row_county in county[0]:\n",
    "                temp.append(county[1]) # Mortality rate percentage\n",
    "                temp.append(county[2]) # Predifined mortality rate category\n",
    "\n",
    "        trimmed_dataset.append(temp)\n",
    "\n",
    "    except IndexError:\n",
    "        continue\n",
    "\n",
    "# Write out trimmed dataset to file\n",
    "with open('Converted_Datasets/trimmed.csv', 'w', newline='') as csvfile:\n",
    "    out = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row in trimmed_dataset:\n",
    "        out.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess.py\n",
    "\n",
    "**Preprocess.py creates a dictionary and checks all of the district schools.**\n",
    "In this way we know that we found matches for all of the school's identifying IRNs. We are then able to cross reference multiple data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load preprocess.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# import csv\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.neural_network import MLPClassifier  \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv(\"Converted_Datasets/trimmed.csv\", sep=',', quotechar='\"')\n",
    "    data.set_index('IRN', inplace=True)\n",
    "\n",
    "    # Assign data from first four columns to X variable\n",
    "    X = data.iloc[:, 1:12]\n",
    "\n",
    "    # Assign data from first fifth columns to y variable    \n",
    "    y = data.iloc[:, 13:]\n",
    "\n",
    "    # le = preprocessing.LabelEncoder()\n",
    "    # y = y.apply(le.fit_transform)  \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)  \n",
    "     \n",
    "    scaler = StandardScaler()  \n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train = scaler.transform(X_train)  \n",
    "    X_test = scaler.transform(X_test) \n",
    "    \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)  \n",
    "    mlp.fit(X_train, y_train.values.ravel())   \n",
    "\n",
    "    predictions = mlp.predict(X_test) \n",
    "\n",
    "    print(mlp.score(X, y))\n",
    "    print(confusion_matrix(y_test,predictions))  \n",
    "    print(classification_report(y_test,predictions)) \n",
    "\n",
    "# create the RFE model and select 3 attributes\n",
    "# rfe = RFE(model, 3)\n",
    "# rfe = rfe.fit(dataset.data, dataset.target)\n",
    "# # summarize the selection of the attributes\n",
    "# print(rfe.support_)\n",
    "# print(rfe.ranking_)\n",
    "\n",
    "    rfe = RFE(mlp, 11)\n",
    "    rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "    # print(rfe.support_)\n",
    "    print(rfe.ranking_)\n",
    "\n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
